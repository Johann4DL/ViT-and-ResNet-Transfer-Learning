{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT - Vision Transformer\n",
    "\n",
    "* $\\textbf{Vision Transformer (ViT)}$ model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.\n",
    "\n",
    "* Fine tuned on $\\textbf{custom data set}$\n",
    "\n",
    "* To do: Train model from scratch (model config - adjust patch size, dropout, ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "EVAL_BATCH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Uncomment below cell to resize the images to 224x224 to match $\\textbf{google/vit-base-patch16-224}$.\n",
    "\n",
    "We have to specify each directory seperately and run the below cell multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# path = \"./train/sit/\"\n",
    "# save_path = \"./train_resized/sit/\"\n",
    "\n",
    "# for root, subdirs, files in os.walk(path):\n",
    "#     for f in files:\n",
    "#         if f.endswith('jpeg'):\n",
    "#             #print(f)\n",
    "#             im = Image.open(path+f)\n",
    "#             imResize = im.resize((224,224), Image.ANTIALIAS)\n",
    "#             imResize.save(save_path + f, 'JPEG', quality=90)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 732, 87)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# No eval data set to evaluate the model during training, due to data scarcity\n",
    "train_ds = torchvision.datasets.ImageFolder('./train_resized', transform=ToTensor())\n",
    "test_ds = torchvision.datasets.ImageFolder('./test_resized', transform=ToTensor())\n",
    "\n",
    "len(train_ds.classes), len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "* To do: cropping, flipping, .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model\n",
    "\n",
    "* To do: Implement ViT from huggingface - use model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ViTConfig, ViTModel\n",
    "\n",
    "# # Initializing a ViT vit-base-patch16-224 style configuration\n",
    "# configuration = ViTConfig(patch_size=32)\n",
    "\n",
    "# # Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n",
    "# test_model = ViTModel(configuration)\n",
    "\n",
    "# # Accessing the model configuration\n",
    "# configuration = test_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust $\\textbf{google/vit-base-patch16-224}$\n",
    "\n",
    "We add a dropout and a classification layer to the model to predict the classes (4 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=4):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "          return logits, loss.item()\n",
    "        else:\n",
    "          return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = ViTForImageClassification(len(train_ds.classes)) \n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Cross Entropy Loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Use GPU if available  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "if torch.cuda.is_available():\n",
    "    model.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train samples: \", len(train_ds))\n",
    "print(\"Number of test samples: \", len(test_ds))\n",
    "print(\"Detected Classes are: \", train_ds.class_to_idx) \n",
    "\n",
    "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) \n",
    "\n",
    "# Train the model\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "#for epoch in range(EPOCHS):  \n",
    "#for epoch in tqdm(range(EPOCHS)):      \n",
    "  for step, (x, y) in enumerate(train_loader):\n",
    "    model.train()\n",
    "    # Change input array into list with each batch being one element\n",
    "    x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
    "    # Remove unecessary dimension\n",
    "    for index, array in enumerate(x):\n",
    "      x[index] = np.squeeze(array)\n",
    "    # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
    "    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "    # Send to GPU if available\n",
    "    x, y  = x.to(device), y.to(device)\n",
    "    b_x = Variable(x)   # batch x (image)\n",
    "    b_y = Variable(y)   # batch y (target)\n",
    "    # Feed through model\n",
    "    output, loss = model(b_x, None)\n",
    "    # Calculate loss\n",
    "    if loss is None: \n",
    "      loss = loss_func(output, b_y)   \n",
    "      optimizer.zero_grad()           \n",
    "      loss.backward()                 \n",
    "      optimizer.step()\n",
    "  \n",
    "  # with torch.no_grad():\n",
    "  #   model.eval()\n",
    "  #   # Get the next batch for testing purposes\n",
    "  #   test = next(iter(test_loader))\n",
    "  #   test_x = test[0]\n",
    "  #   # Reshape and get feature matrices as needed\n",
    "  #   test_x = np.split(np.squeeze(np.array(test_x)), BATCH_SIZE)\n",
    "  #   for index, array in enumerate(test_x):\n",
    "  #     test_x[index] = np.squeeze(array)\n",
    "  #   test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\n",
    "  #   # Send to appropirate computing device\n",
    "  #   test_x = test_x.to(device)\n",
    "  #   test_y = test[1].to(device)\n",
    "  #   # Get output (+ respective class) and compare to target\n",
    "  #   test_output, loss = model(test_x, test_y)\n",
    "  #   test_output = test_output.argmax(1)\n",
    "  #   # Calculate Accuracy\n",
    "  #   accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\n",
    "  #   print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| test accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "\n",
    "# Disable grad\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        # inputs, target = next(iter(test_loader))\n",
    "        # Reshape and get feature matrices as needed\n",
    "        # print(inputs.shape)\n",
    "        inputs = inputs[0].permute(1, 2, 0)\n",
    "        # print(inputs.shape)\n",
    "        # Save original Input\n",
    "        # originalInput = inputs\n",
    "        # for index, array in enumerate(inputs):\n",
    "        #   inputs[index] = np.squeeze(array)\n",
    "        #   print(inputs[index].shape)\n",
    "        inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
    "        # print(inputs.shape)\n",
    "        # print(targets.shape)\n",
    "\n",
    "        # Send to appropriate computing device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "      \n",
    "        # Generate prediction\n",
    "        prediction, loss = model(inputs, targets)\n",
    "          \n",
    "        # Predicted class value using argmax\n",
    "        predicted_class = np.argmax(prediction.cpu())\n",
    "        value_predicted = list(test_ds.class_to_idx.keys())[list(test_ds.class_to_idx.values()).index(predicted_class)]\n",
    "        value_target = list(test_ds.class_to_idx.keys())[list(test_ds.class_to_idx.values()).index(targets)]\n",
    "        accuracy = (predicted_class == targets).sum().item() / EVAL_BATCH\n",
    "        acc.append(accuracy)\n",
    "\n",
    "        # if step % 10 == 0:     \n",
    "        #     # Show result\n",
    "        #     plt.imshow(originalInput)\n",
    "        #     # plt.xlim(224,0)\n",
    "        #     # plt.ylim(224,0)\n",
    "        #     plt.title(f'Prediction: {value_predicted} - Actual target: {value_target}')\n",
    "        #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(acc) / len(acc) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for 10 epochs\n",
    "\n",
    "* Model 1:  93.10 \n",
    "* Model 2:  91.95\n",
    "* Model 3:  94.25\n",
    "* Model 4:  91.95\n",
    "* Model 5:  91.95\n",
    "* Model 6:  93.10\n",
    "* Model 7:  93.10\n",
    "* Model 8:  91.95\n",
    "* Model 9:  91.95\n",
    "* Model 10: 93.10\n",
    "\n",
    "Average accuracy: 92.64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model\n",
    "\n",
    "We save and provide the best model with an accuracy of 94.25 on he test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(model, './models/best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = './models/best_model.pt'\n",
    "# model = torch.load(MODEL_PATH)\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
