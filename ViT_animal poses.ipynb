{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT - Vision Transformer\n",
    "\n",
    "* $\\textbf{Vision Transformer (ViT)}$ model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.\n",
    "\n",
    "* Fine tuned on $\\textbf{custom data set}$\n",
    "\n",
    "* To do: Train model from scratch (model config - adjust patch size, dropout, ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "EVAL_BATCH = 1\n",
    "\n",
    "PRETRAINED = True\n",
    "\n",
    "AUGMENTATION = False  # data augmentation decreses the overall accuracy\n",
    "AUGS = 1\n",
    "\n",
    "GENDATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Uncomment the cell below to resize the images to 224x224 to match $\\textbf{google/vit-base-patch16-224}$.\n",
    "\n",
    "We have to specify each directory seperately and run the below cell multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# path = \"./train/sit/\"\n",
    "# save_path = \"./train_resized/sit/\" # specify a directory for each class: sit, kie, run, walk/stand\n",
    "\n",
    "# for root, subdirs, files in os.walk(path):\n",
    "#     for f in files:\n",
    "#         if f.endswith('jpeg'):\n",
    "#             #print(f)\n",
    "#             im = Image.open(path+f)\n",
    "#             imResize = im.resize((224,224), Image.ANTIALIAS)\n",
    "#             imResize.save(save_path + f, 'JPEG', quality=90)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  4\n",
      "Training samples:  1180\n",
      "Test samples:  87\n",
      "\n",
      "Samples per class in training set:\n",
      "Class lie:  275\n",
      "Class run:  213\n",
      "Class sit:  161\n",
      "Class walk_stand:  531\n",
      "\n",
      "Samples per class in test set:\n",
      "Class lie:  10\n",
      "Class run:  8\n",
      "Class sit:  8\n",
      "Class walk_stand:  61\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, v2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import ViTConfig, ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "if GENDATA == False:\n",
    "    # No eval data set to evaluate the model during training, due to data scarcity\n",
    "    train_ds = torchvision.datasets.ImageFolder('./data/train_resized', transform=ToTensor())  # data is not on GitHub -> change data path\n",
    "    test_ds = torchvision.datasets.ImageFolder('./data/test_resized', transform=ToTensor())\n",
    "else:\n",
    "    train_ds = torchvision.datasets.ImageFolder('./data/Stable_Diffusion_data/train_resized', transform=ToTensor())\n",
    "    test_ds = torchvision.datasets.ImageFolder('./data/test_resized', transform=ToTensor())\n",
    "\n",
    "print('Number of classes: ', len(train_ds.classes)), print('Training samples: ', len(train_ds)), print('Test samples: ', len(test_ds))\n",
    "# samples per class\n",
    "print('\\nSamples per class in training set:')\n",
    "for i in range(len(train_ds.classes)):\n",
    "    print('Class {}: '.format(train_ds.classes[i]), len([x for x in train_ds.targets if x == i]))\n",
    "\n",
    "print('\\nSamples per class in test set:')\n",
    "for i in range(len(test_ds.classes)):\n",
    "    print('Class {}: '.format(test_ds.classes[i]), len([x for x in test_ds.targets if x == i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "* To do: cropping, flipping, .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data augmentation applied.\n"
     ]
    }
   ],
   "source": [
    "# data augmentation for training data\n",
    "\n",
    "if AUGMENTATION == True:\n",
    "\n",
    "    augmentations = AUGS # 1\n",
    "\n",
    "    for i in range(augmentations):\n",
    "\n",
    "        augmented_train_data = torchvision.datasets.ImageFolder('./data/train_resized', transform=v2.Compose([\n",
    "            v2.RandomResizedCrop(size = (224,224), antialias=True),\n",
    "            v2.RandomRotation(degrees=15),\n",
    "            #v2.RandomVerticalFlip(), \n",
    "            v2.RandomHorizontalFlip(),\n",
    "            v2.ToTensor(),\n",
    "            #v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]))\n",
    "\n",
    "        train_ds = data.ConcatDataset([train_ds, augmented_train_data])\n",
    "        print('We added augmented data to the training set. The new length of the training set is: ', len(train_ds))\n",
    "else:\n",
    "    print('No data augmentation applied.')\n",
    "\n",
    "# no data augmentation for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model\n",
    "\n",
    "* To do: Implement ViT from huggingface - use model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a ViT vit-base-patch16-224 style configuration\n",
    "configuration = ViTConfig(patch_size=16)\n",
    "\n",
    "# Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n",
    "test_model = ViTModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = test_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust $\\textbf{google/vit-base-patch16-224}$\n",
    "\n",
    "We add a dropout and a classification layer to the model to predict the classes (4 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=4):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        if PRETRAINED == True:     \n",
    "          self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "          # freeze the weights\n",
    "          # for param in self.vit.parameters():\n",
    "          #   param.requires_grad = False\n",
    "          print('Model initialized with pre-trained weights.')\n",
    "        else:\n",
    "          self.vit = ViTModel(configuration)\n",
    "          print('\\nModel initialized with random weights.')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "          return logits, loss.item()\n",
    "        else:\n",
    "          return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with pre-trained weights.\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = ViTForImageClassification(4) #len(train_ds.classes)) \n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples:  1180\n",
      "Number of test samples:  87\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train samples: \", len(train_ds))\n",
    "print(\"Number of test samples: \", len(test_ds))\n",
    "#print(\"Detected Classes are: \", train_ds.class_to_idx) \n",
    "\n",
    "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) \n",
    "\n",
    "# Train the model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "#for epoch in range(EPOCHS):  \n",
    "#for epoch in tqdm(range(EPOCHS)):      \n",
    "  for step, (x, y) in enumerate(train_loader):\n",
    "    model.train()\n",
    "    # Change input array into list with each batch being one element\n",
    "    x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
    "    # Remove unecessary dimension\n",
    "    for index, array in enumerate(x):\n",
    "      x[index] = np.squeeze(array)\n",
    "    # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
    "    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "    # Send to GPU if available\n",
    "    x, y  = x.to(device), y.to(device)\n",
    "    b_x = Variable(x)   # batch x (image)\n",
    "    b_y = Variable(y)   # batch y (target)\n",
    "    # Feed through model\n",
    "    output, loss = model(b_x, None)\n",
    "    # Calculate loss\n",
    "    if loss is None: \n",
    "      loss = loss_func(output, b_y)   \n",
    "      optimizer.zero_grad()           \n",
    "      loss.backward()                 \n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.10344827586206\n",
      "\n",
      "Total over all classes:  81  out of  87\n",
      "Class lie:  10  out of  10\n",
      "Class run:  6  out of  8\n",
      "Class sit:  7  out of  8\n",
      "Class walk_stand:  58  out of  61\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "class_1 = 0\n",
    "class_2 = 0\n",
    "class_3 = 0\n",
    "class_4 = 0\n",
    "\n",
    "# confusion_matrix = torch.zeros(4, 4)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs[0].permute(1, 2, 0)\n",
    "        inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
    "\n",
    "        # Send to appropriate computing device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "      \n",
    "        # Generate prediction\n",
    "        prediction, loss = model(inputs, targets)\n",
    "          \n",
    "        # Predicted class value using argmax\n",
    "        predicted_class = np.argmax(prediction.cpu())\n",
    "        value_predicted = list(test_ds.class_to_idx.keys())[list(test_ds.class_to_idx.values()).index(predicted_class)]\n",
    "        value_target = list(test_ds.class_to_idx.keys())[list(test_ds.class_to_idx.values()).index(targets)]\n",
    "        # confusion matrix\n",
    "        if predicted_class == targets:\n",
    "            if predicted_class == 0:\n",
    "                class_1 += 1\n",
    "            elif predicted_class == 1:\n",
    "                class_2 += 1\n",
    "            elif predicted_class == 2:\n",
    "                class_3 += 1\n",
    "            elif predicted_class == 3:\n",
    "                class_4 += 1\n",
    "        accuracy = (predicted_class == targets).sum().item() / EVAL_BATCH\n",
    "        acc.append(accuracy)\n",
    "\n",
    "        # for t, p in zip(targets.view(-1), prediction.view(-1)):\n",
    "        #     confusion_matrix[t.long(), p.long()] += 1\n",
    "        #     false_positives = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "        #     false_negatives = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "        #     true_positives = np.diag(confusion_matrix)\n",
    "        #     true_negatives = confusion_matrix.sum() - (false_positives + false_negatives + true_positives)\n",
    "\n",
    "\n",
    "        # # calculate confusion matrix\n",
    "        # confusion_matrix = torch.zeros(4, 4)\n",
    "        # for t, p in zip(targets.view(-1), prediction.view(-1)):\n",
    "        #     confusion_matrix[t.long(), p.long()] += 1\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "print('Accuracy: ', np.mean(acc)*100)\n",
    "print('\\nTotal over all classes: ', class_1 + class_2 + class_3 + class_4, ' out of ', len(test_ds))\n",
    "print('Class {}: '.format(test_ds.classes[0]), class_1, ' out of ', len([x for x in test_ds.targets if x == 0]))\n",
    "print('Class {}: '.format(test_ds.classes[1]), class_2, ' out of ', len([x for x in test_ds.targets if x == 1]))\n",
    "print('Class {}: '.format(test_ds.classes[2]), class_3, ' out of ', len([x for x in test_ds.targets if x == 2]))\n",
    "print('Class {}: '.format(test_ds.classes[3]), class_4, ' out of ', len([x for x in test_ds.targets if x == 3]))\n",
    "# print('Confusion matrix: ')\n",
    "# print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model from scratch\n",
    "\n",
    "#### no data augmentation, no synthetic data\n",
    "\n",
    "* Model 1:  71.26 \n",
    "* Model 2:  71.26\n",
    "* Model 3:  66.67\n",
    "* Model 4:  70.11\n",
    "* Model 5:  67.82\n",
    "\n",
    "Total: 69.42\n",
    "\n",
    "#### with data augmentation, no synthetic data\n",
    "\n",
    "* Model 1:  70.11 \n",
    "* Model 2:  70.11\n",
    "* Model 3:  70.11\n",
    "\n",
    "Total: \n",
    "\n",
    "Confusion Matrix example: \n",
    "Total over all classes:  62  out of  87\n",
    "Class lie:  1  out of  10\n",
    "Class run:  0  out of  8\n",
    "Class sit:  1  out of  8\n",
    "Class walk_stand:  60  out of  61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model pretrained on ImageNet\n",
    "\n",
    "#### NO data augmentation and NO synthetic data  (10 epochs)\n",
    "\n",
    "* Model 1:  93.10 \n",
    "* Model 2:  91.95\n",
    "* Model 3:  94.25\n",
    "* Model 4:  91.95\n",
    "* Model 5:  91.95\n",
    "* Model 6:  93.10\n",
    "* Model 7:  93.10\n",
    "* Model 8:  91.95\n",
    "* Model 9:  91.95\n",
    "* Model 10: 93.10\n",
    "\n",
    "Average accuracy: 92.64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model pretrained on ImageNet\n",
    "\n",
    "#### WITH data augmentation and NO synthetic data\n",
    "\n",
    "* Model 1:  86.21 \n",
    "* Model 2:  87.36\n",
    "* Model 3:  89.66\n",
    "* Model 4:  89.66\n",
    "\n",
    "Total: 88.22\n",
    "\n",
    "Total over all classes:  78  out of  87\n",
    "Class lie:  7  out of  10\n",
    "Class run:  6  out of  8\n",
    "Class sit:  6  out of  8\n",
    "Class walk_stand:  59  out of  61    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO data augmentation and WITH synthetic data\n",
    "\n",
    "* Model 1:  94.25 \n",
    "* Model 2:  95.40\n",
    "* Model 3:  94.25 \n",
    "\n",
    "Total over all classes:  82  out of  87\n",
    "Class lie:  10  out of  10\n",
    "Class run:  6  out of  8\n",
    "Class sit:  7  out of  8\n",
    "Class walk_stand:  59  out of  61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model\n",
    "\n",
    "We save and provide the best model with an accuracy of 94.25 on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(model, './models/best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = './models/best_model.pt'\n",
    "# model = torch.load(MODEL_PATH)\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
