{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Features, ClassLabel, Array3D\n",
    "from transformers import ViTFeatureExtractor, default_data_collator\n",
    "from transformers import ViTForImageClassification\n",
    "from transformers import ViTModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [00:00<00:00, 368411.85it/s]\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:00<00:00, 209336.88it/s]\n",
      "Using custom data configuration default-0d3c020d50177a97\n",
      "Found cached dataset imagefolder (/home/johann/.cache/huggingface/datasets/imagefolder/default-0d3c020d50177a97/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 302.65it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir=\"./\")\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = train_ds.train_test_split(test_size=0.1)\n",
    "\n",
    "train_ds = splits['train']\n",
    "\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(decode=True, id=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = Features({\n",
    "    'label': ClassLabel(names=['lie', 'run', 'sit', 'walk_stand']),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3,32,32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "train_ds.features['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = test_ds.train_test_split(test_size=0.01)\n",
    "test_ds = test_split['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21962/3911851435.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"f1\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/ba]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.25ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.21ba/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_images(examples):\n",
    "    \n",
    "    images = examples['image']\n",
    "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "    images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n",
    "    inputs = feature_extractor(images=images)\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "\n",
    "    return examples\n",
    "\n",
    "features = Features({\n",
    "    'label': ClassLabel(names=['lie', 'run', 'sit', 'walk_stand']),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3,32,32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "preprocessed_train_ds = train_ds.map(preprocess_images, batched=True)\n",
    "preprocessed_val_ds = val_ds.map(preprocess_images, batched=True)\n",
    "preprocessed_test_ds = test_ds.map(preprocess_images, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (shared_parameters): ModuleDict()\n",
       "  (vit): ViTModel(\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (1): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (2): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (3): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (4): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (5): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (6): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (7): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (8): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (9): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (10): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (11): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_adapters): AdapterLayer(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (attention_adapters): AdapterLayer(\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "# The ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes, \n",
    "# and fine-tuned on ImageNet, a dataset consisting of 1 million images and 1k classes.\n",
    "# print(model.config)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForImageClassification2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels=4):\n",
    "\n",
    "        super(ViTForImageClassification2, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs)\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    'test-vit',\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric,\n",
    "    logging_dir='logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = preprocessed_train_ds,\n",
    "    eval_dataset = preprocessed_val_ds,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
